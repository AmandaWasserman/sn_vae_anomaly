{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py, os\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.style.use('classic')\n",
    "font = font_manager.FontProperties(family='serif', size=16)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h5_loader(file_path: str='background_for_training.h5', min_pt=None, max_pt=None, mean_eta=None, std_eta=None, mean_phi=None, std_phi=None, min_class=None, max_class=None):\n",
    "    ff = h5py.File(file_path, 'r')\n",
    "    particles = np.asarray(ff.get('Particles'))\n",
    "    particles = normalize_features(particles, min_pt, max_pt, mean_eta, std_eta, mean_phi, std_phi, min_class, max_class).reshape((-1, 19*4))\n",
    "    particles = torch.from_numpy(particles)\n",
    "    print(particles.shape)\n",
    "    return particles\n",
    "\n",
    "def summary_statistics(file_path: str='background_for_training.h5'):\n",
    "    ff = h5py.File(file_path, 'r')\n",
    "    particles = np.asarray(ff.get('Particles'))\n",
    "    idx_pt, idx_eta, idx_phi, idx_class = range(4)\n",
    "    min_pt    = np.min(particles[:,:,idx_pt])\n",
    "    max_pt    = np.max(particles[:,:,idx_pt])\n",
    "    mean_eta  = np.mean(particles[:,:,idx_eta])\n",
    "    std_eta   = np.std(particles[:,:,idx_eta])\n",
    "    mean_phi  = np.mean(particles[:,:,idx_phi])\n",
    "    std_phi   = np.std(particles[:,:,idx_phi])\n",
    "    min_class = np.min(particles[:,:,idx_class])\n",
    "    max_class = np.max(particles[:,:,idx_class])\n",
    "    return min_pt, max_pt, mean_eta, std_eta, mean_phi, std_phi, min_class, max_class\n",
    "\n",
    "def normalize_features(particles, min_pt=None, max_pt=None, mean_eta=None, std_eta=None, mean_phi=None, std_phi=None, min_class=None, max_class=None):\n",
    "    idx_pt, idx_eta, idx_phi, idx_class = range(4)\n",
    "    if min_pt==None: min_pt    = np.min(particles[:,:,idx_pt])\n",
    "    if max_pt==None: max_pt    = np.max(particles[:,:,idx_pt])\n",
    "    if mean_eta==None: mean_eta  = np.mean(particles[:,:,idx_eta])\n",
    "    if std_eta==None: std_eta   = np.std(particles[:,:,idx_eta])\n",
    "    if mean_phi==None: mean_phi  = np.mean(particles[:,:,idx_phi])\n",
    "    if std_phi==None: std_phi   = np.std(particles[:,:,idx_phi])\n",
    "    if min_class==None: min_class = np.min(particles[:,:,idx_class])\n",
    "    if max_class==None: max_class = np.max(particles[:,:,idx_class])\n",
    "    # min-max normalize pt\n",
    "    particles[:,:,idx_pt] = (particles[:,:,idx_pt] - min_pt) / (max_pt-min_pt)\n",
    "    # standard normalize angles\n",
    "    particles[:,:,idx_eta] = (particles[:,:,idx_eta] - mean_eta)/std_eta\n",
    "    particles[:,:,idx_phi] = (particles[:,:,idx_phi] - mean_phi)/std_phi\n",
    "    # min-max normalize class label\n",
    "    particles[:,:,idx_class] = (particles[:,:,idx_class] - min_class) / (max_class-min_class)\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    encoder produces mean and log of variance \n",
    "    (i.e., parateters of simple tractable normal distribution \"q\"\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.FC_input = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.FC_hidden = nn.ModuleList([nn.Linear(hidden_dim[i], hidden_dim[i+1]) for i in range(len(hidden_dim)-1)])\n",
    "        #self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_mean  = nn.Linear(hidden_dim[-1], latent_dim)\n",
    "        self.FC_var   = nn.Linear (hidden_dim[-1], latent_dim)\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        self.training = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.LeakyReLU(self.FC_input(x))\n",
    "        for i, FC in enumerate(self.FC_hidden):\n",
    "            h = self.LeakyReLU(FC(h))\n",
    "        mean = self.FC_mean(h)\n",
    "        log_var = self.FC_var(h)\n",
    "        return mean, log_var\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.FC_latent = nn.Linear(latent_dim, hidden_dim[0])\n",
    "        self.FC_hidden = nn.ModuleList([nn.Linear(hidden_dim[i], hidden_dim[i+1]) for i in range(len(hidden_dim)-1)])\n",
    "        self.FC_output = nn.Linear(hidden_dim[-1], output_dim)\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.LeakyReLU(self.FC_latent(x))\n",
    "        for i, FC in enumerate(self.FC_hidden):\n",
    "            h = self.LeakyReLU(FC(h))\n",
    "        x_hat = torch.sigmoid(self.FC_output(h))\n",
    "        return x_hat\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dim_encoder, hidden_dim_decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.Encoder = Encoder(input_dim=input_dim, hidden_dim=hidden_dim_encoder, latent_dim=latent_dim)\n",
    "        self.Decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim_decoder, output_dim=input_dim)\n",
    "        \n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(DEVICE)  # sampling epsilon        \n",
    "        z = mean + var*epsilon       # reparameterization trick\n",
    "        return z\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.Encoder(x)\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -> var)\n",
    "        x_hat = self.Decoder(z)\n",
    "        return x_hat, mean, log_var\n",
    "\n",
    "def AllLoss(x, x_hat, mean, log_var, lambda_kld):\n",
    "    Reco = RecoLoss(x_hat, x)\n",
    "    KLD  = KLDLoss(mean, log_var)\n",
    "    return Reco + lambda_kld*KLD, Reco, lambda_kld*KLD\n",
    "\n",
    "def RecoLoss(x_hat, x, reduction='sum'):\n",
    "    return nn.functional.binary_cross_entropy(x_hat, x, reduction=reduction)\n",
    "    \n",
    "def KLDLoss(mean, log_var, reduction='sum'):\n",
    "    if reduction=='sum':\n",
    "        return - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "    elif reduction=='none':\n",
    "        return - 0.5 * (1+ log_var - mean.pow(2) - log_var.exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabel = [r'$p_{\\rm T}$', r'$\\eta$', r'$\\phi$', 'class']\n",
    "obj_label = ['MET', \n",
    "             'lepton 1', 'lepton 2', 'lepton 3', 'lepton 4', \n",
    "             'electron 1', 'electron 2', 'electron 3', 'electron 4',\n",
    "             'jet 1', 'jet 2', 'jet 3', 'jet 4', 'jet 5', 'jet 6', 'jet 7', 'jet 8', 'jet 9', 'jet 10'\n",
    "        ]\n",
    "bins = [\n",
    "    np.linspace(0, 0.5, 30),\n",
    "    np.linspace(-6, 6, 30),\n",
    "    np.linspace(-6, 6, 30),\n",
    "    np.linspace(0, 1, 30),\n",
    "       ]\n",
    "sm= test_dataset[0]\n",
    "sm=sm.view(-1, sm.shape[1]).float()\n",
    "sm=sm.cpu()\n",
    "for i in range(19):\n",
    "    fig = plt.figure(figsize=(15, 4))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    for j in range(4):\n",
    "        ax = fig.add_axes([0.05+0.24*j+(i*4), 0.2, 0.21, 0.75])\n",
    "        h = plt.hist(sm[:, j+(i*4)], density=True, color='black', bins=bins[j], lw=2, histtype='step', label='SM')\n",
    "        ax.set_yscale('log')\n",
    "        plt.xlabel(obj_label[i]+' '+xlabel[j], fontsize=16, fontname='serif')\n",
    "        plt.xticks(fontsize=12, fontname='serif')\n",
    "        plt.yticks(fontsize=12, fontname='serif')\n",
    "        \n",
    "        if j==0:\n",
    "            plt.ylabel('probability', fontsize=16, fontname='serif')\n",
    "            font = font_manager.FontProperties(family='serif', size=14)\n",
    "            plt.xscale('log')\n",
    "        if j==3:\n",
    "            ax.legend(prop=font, loc='best', frameon=False)\n",
    "    #plt.savefig('./input_features_obj%i.pdf'%(i))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
